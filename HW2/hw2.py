# -*- coding: utf-8 -*-
"""hw2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-SbcVeoyNTN4HvHIyyCDiV2JmzhzvGWt
"""

# Connetced yourself Cloud Storage
from google.colab import drive
drive.mount('/content/gdrive/', force_remount=True)

import os
os.chdir("/content/gdrive/My Drive/")
os.listdir()

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# # Ubuntu no longer distributes chromium-browser outside of snap
# #
# # Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap
# 
# # Add debian buster
# cat > /etc/apt/sources.list.d/debian.list <<'EOF'
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main
# deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main
# EOF
# 
# # Add keys
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138
# apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A
# 
# apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg
# apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg
# apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg
# 
# # Prefer debian repo for chromium* packages only
# # Note the double-blank lines between entries
# cat > /etc/apt/preferences.d/chromium.pref << 'EOF'
# Package: *
# Pin: release a=eoan
# Pin-Priority: 500
# 
# Package: *
# Pin: origin "deb.debian.org"
# Pin-Priority: 300
# 
# 
# Package: chromium*
# Pin: origin "deb.debian.org"
# Pin-Priority: 700
# EOF
# 
# # Install chromium and chromium-driver
# apt-get update
# apt-get install chromium chromium-driver
# 
# # Install selenium
# pip install selenium

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import pandas as pd
import logging

# Configure logging module
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Constants definition
URL = 'https://pleagueofficial.com/stat-player'
OUTPUT_FILE = 'plg.csv'
WAIT_TIME = 10

def scrape_plg_data(url=URL, output_file=OUTPUT_FILE, wait_time=WAIT_TIME):
    try:
        # Create Chrome WebDriver with headless mode and other options
        chrome_options = webdriver.ChromeOptions()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')

        # Initialize Chrome WebDriver, passing the path of Chrome driver and options
        with webdriver.Chrome(options=chrome_options) as driver:
            # Load the target webpage
            driver.get(url)
            logging.info('Web page loaded successfully.')

            # Wait for the webpage to load completely
            driver.implicitly_wait(wait_time)

            # Get the webpage content
            html = driver.page_source

        # Parse HTML using Beautiful Soup
        soup = BeautifulSoup(html, 'html.parser')

        # Find the table element
        table = soup.find('table')

        if table:
            # Use pandas read_html to parse the table
            dfs = pd.read_html(str(table), header=0, keep_default_na=False)

            if dfs:
                # Select the first table
                selected_table = dfs[0]

                # Save data as a CSV file
                selected_table.to_csv(output_file, index=False)
                logging.info(f'Data saved to {output_file}.')

                # Return the selected table
                return selected_table
            else:
                logging.error('No tables found in the HTML.')
        else:
            logging.error('Table element not found in the HTML.')

    except TimeoutException:
        logging.error('Timeout error occurred while loading the web page.')
    except NoSuchElementException:
        logging.error('Element not found error occurred while parsing the HTML.')
    except Exception as e:
        logging.error(f'An error occurred: {str(e)}')

    return None

if __name__ == "__main__":
    scraped_data = scrape_plg_data()
    if scraped_data is not None:
        print(scraped_data)